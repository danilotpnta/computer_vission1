{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7PRKfXw99hy"
   },
   "source": [
    "<center><img src=\"https://miro.medium.com/v2/resize:fit:1250/format:webp/1*QgI1t-7yJApi4vQigFgsLQ.jpeg\" width=30% ></center>\n",
    "\n",
    "# <center> Lab Project Part 1: Image Classification using Bag-of-Words </center>\n",
    "<center> Computer Vision 1, University of Amsterdam </center>\n",
    "<center> Due 23:59, October 21, 2023 (Amsterdam time) </center>\n",
    "\n",
    "***\n",
    "\n",
    "<center>\n",
    "<b>TA's: Qi Bi, Avik Pal, Nimi Barazani</b>\n",
    "\n",
    "Student1 ID: \\\n",
    "Student1 Name:\n",
    "\n",
    "Student2 ID: \\\n",
    "Student2 Name:\n",
    "\n",
    "Student3 ID: \\\n",
    "Student3 Name:\n",
    "\n",
    "( Student4 ID: \\\n",
    "Student4 Name: )\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3TPoO70b2iE"
   },
   "source": [
    "# **Instructions**\n",
    "\n",
    "1. For both parts of the final project, students are expected to prepare a report. The report should include answers to all questions, written details on implementation approaches, the analysis of the results for different settings and visualizations to illustrate experiments with and performance of your implementation. Grading will primarily be based on the report (i.e. it should be self-contained as much as possible). If the report contains any faulty results or ambiguities, the TA's can take a look at your code to find out what happened.\n",
    "\n",
    "2. Do not just provide numbers without explanation, remember to follow the general guidelines and discuss different settings to show you understand the material and the processes at work.\n",
    "\n",
    "3. For qualitative evaluation, you are expected to visualize the top-5 and the bottom-5 ranked test images (based on the classifier confidence for the target class) per setup. That means you are supposed to provide a figure for each experimental setup, as discussed in Section 2.6.\n",
    "\n",
    "**Hint:** Having visual elements such as charts, graphs and plots are always useful for everyone. Keep this in mind while writing your reports.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_z5OJOVsv23"
   },
   "source": [
    "# **General Guidelines**\n",
    "1. Aim:\n",
    "    - Able to understand the basic Image Recognition/Classification pipeline using a traditional Bag of Words method.\n",
    "    - Able to use to python packages for image classification: *matplotlib, cv2, sklearn etc.*\n",
    "2. Prerequisite:\n",
    "    - Familiarity with Python and relevant packages.\n",
    "    - Know the basics of feature descriptors (SIFT, HoG) and machine learning tools (K-means, SVM and etc.).\n",
    "3. Guidelines:\n",
    "    Students should work on the assignments in their assignment group for **two** weeks.\n",
    "\n",
    "    Any questions regarding the assignment content can be discussed on Piazza.\n",
    "    \n",
    "    Your source code and report must be handed in together in a zip file (**ID1_ID2_ID3_part1.zip**) before the deadline. Make sure your report follows these guidelines:\n",
    "    - *The maximum number of pages for this part is 10 (single-column, including tables and figures). Please express your thoughts concisely.*\n",
    "    - *Follow the given instructions and answer all given questions. Briefly describe what you implemented.*\n",
    "    - *Show you understand the algorithms and implementations: explain why certain settings produce certain results. When constructing graphs, tables and other figures, make your figures as informative as possible (choose relevant sample sizes, axes, etc.), to illustrate your arguments*\n",
    "    - *Tables and figures must be accompanied by a brief description. Do not forget to add a number, a title, and if applicable name and unit of variables in a table, name and unit of axes and legends in a figure.*\n",
    "\n",
    "4. The report should be handed in in **PDF-format**. Your code should be handed in in **.ipynb format**. This does not mean you have to make your project in a notebook, it just means it **should be submitted as a notebook**. Be sure to test whether all your functionality works as expected when ran in a notebook. If you use a Conda environment, be sure to include it in your submission.\n",
    "\n",
    "5. **Late submissions** are not allowed. Assignments that are submitted after the strict deadline will not be graded. In case of submission conflicts, TAs' system clock is taken as reference. We strongly recommend submitting well in advance, to avoid last minute system failure issues.\n",
    "6. **Plagiarism note**:\n",
    "Keep in mind that plagiarism (submitted materials which are not your work) is a serious crime and any misconduct shall be punished with the university regulations.\n",
    "This includes the use of ChatGPT and other generative AI tools.\n",
    "\n",
    "<!-- ### PyTorch versions\n",
    "we assume that you are using latest PyTorch version(>=1.4)\n",
    "\n",
    "### PyTorch Tutorial & Docs\n",
    "This tutorial aims to make you familiar with the programming environment that will be used throughout the course. If you have experience with PyTorch or other frameworks (TensorFlow, MXNet *etc.*), you can skip the tutorial exercises; otherwise, we suggest that you complete them all, as they are helpful for getting hands-on experience.\n",
    "\n",
    "**Anaconda Environment** We recommend installing \\textit{anaconda} for configuring \\textit{python} package dependencies, whereas it's also fine to use other environment managers as you like. The installation of anaconda can be found in [anaconda link](https://docs.anaconda.com/anaconda/install/).\n",
    "\n",
    "**Installation** The installation of PyTorch is available at [install link](https://pytorch.org/get-started/locally/) depending on your device and system.\n",
    "\n",
    "**Getting start** The 60-minute blitz can be found at [blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html), and and examples are at [examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)\n",
    "\n",
    "**Documents** There might be potential unknown functions or classes, you shall look through the official documents website ([Docs](https://pytorch.org/docs/stable/index.html)) and figure them out by yourself. (***Think***:} What's the difference between *torch.nn.Conv2d* and *torch.nn.functional.conv2d*?)\n",
    "You can learn pytorch from the [tutorial link](https://pytorch.org/tutorials/). The Docs information can be searched at [Docs](https://pytorch.org/docs/stable/index.html). In this assignments, we wish you to form the basic capability of using one of the well-known   -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7uNsGM0F9rfE"
   },
   "source": [
    "# **1. Introduction**\n",
    "\n",
    "The goal of the assignment is to implement a system for image classification. In other words, this system should tell if there is an object of given class in an image. You will perform 5-class ({1: *airplanes*, 2: *birds*, 3: *ships*, 4: *horses*, 5: *automobiles*}) image classification based on a bag-of-words approach ([reference](http://www.robots.ox.ac.uk/~az/icvss08_az_bow.pdf)) using SIFT features. [CIFAR10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) will be used for the task. For each class, the test sub-directories contain 1000 images, and the training sub-directories contain 5000 images. Images are represented as (RGB) 32x32 pixels.\n",
    "\n",
    "Download the [dataset](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz). The archive contains the files *data_batch_1*, *data_batch_2*, ..., *data_batch_5*, as well as *test_batch*. Download the dataset and make yourself familiar with it by figuring out which images and labels you need for the aforementioned 5 classes.\n",
    "\n",
    "**Note:** You may also opt to work with a subset of the training images if you choose, as long as this subset includes at least 500 images from each of the 5 classes. This option is to allow for a more manageable workload, especially if computational resources are limited. If applicable, consider the effects of using this smaller set of images on the quality of your implementation.\n",
    "\n",
    "**Hint:**\n",
    "In a real scenario, the public data you use often deviates from your task. You need to figure it out and re-arrange the labels as required using the *lab1_utils.py* file (provided in the zip) as a reference. You may (or may not) choose to use the following code to load the dataset:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "z-CKadcuvVPE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from lab1_utils import CIFAR10\n",
    "trainset = CIFAR10(\"./data\", train=True, N=1000)\n",
    "testset = CIFAR10(\"./data\", train=False)\n",
    "\n",
    "training_data = trainset.data\n",
    "training_label = trainset.targets\n",
    "test_data = testset.data\n",
    "test_label = testset.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#  Data is unordered/ Ordered after running np.argsort\n",
    "# print(training_label[0:4])\n",
    "\n",
    "np.random.seed(42)\n",
    "dic_class_labels = {0: 'airplanes', \n",
    "                    1: 'birds', \n",
    "                    2: 'ships', \n",
    "                    3: 'horses', \n",
    "                    4: 'automobiles'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJZPyMT7_Kvz"
   },
   "source": [
    "## **1.1 Training Phase**\n",
    "\n",
    "Training must be conducted over the training set.\n",
    "Later on the training set will be divided into two subsets, one for building the visual vocabulary, and the other for training the classifier.\n",
    "Keep in mind that using more samples in training will likely result in better performance. However, if your computational resources are limited and/or your system is slow, it's OK to use less training data to save time.\n",
    "\n",
    "**Hint:** To debug your code, you can use a small amount of input images/descriptors. Once you are sure everything works properly, you can run your code for the experiment using all the data points.\n",
    "\n",
    "**Hint:** You are not allowed to use the test images for training purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDzqpqAn_ueU"
   },
   "source": [
    "## **1.2 Testing Phase**\n",
    "\n",
    "You have to test your system using the specified subset of test images. All 1000 test images (per class) should be used at once for testing to observe the full performance. Again, exclude them from training for fair comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ma47S1_-_-aN"
   },
   "source": [
    "# **2. Bag-of-Words based Image Classification**\n",
    "\n",
    "Bag-of-Words based Image Classification system contains the following steps:\n",
    "1. Feature extraction and description\n",
    "2. Building a visual vocabulary (use the first subset from the training set)\n",
    "3. Quantify features using visual dictionary (encoding)\n",
    "4. Representing images by frequencies of visual words\n",
    "5. Train the classifier (use the second subset from the training set)\n",
    "\n",
    "We will consider each step in detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXQBnhT499h2"
   },
   "source": [
    "## **2.1 Feature Extraction and Description**\n",
    "\n",
    "SIFT descriptors can be extracted from keypoints. You can use SIFT related functions in *OpenCV* for feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvNbVoMh99h5"
   },
   "source": [
    "####  **` Q2.1: Extract SIFT descriptors from training datasets based on keypoints. Show two images from each of the five classes (draw the circles with the size of keypoints). (10-pts).`**  \n",
    "\n",
    "**Hint:**\n",
    "Check out the Docs of SIFT and related functions for further information in the following [link1](https://docs.opencv.org/master/da/df5/tutorial_py_sift_intro.html) and [link2](https://docs.opencv.org/master/d7/d60/classcv_1_1SIFT.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "47K1Kzph99h3"
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# YOUR CODE HERE\n",
    "######################################################\n",
    "\n",
    "from skimage.feature import hog\n",
    "import cv2\n",
    "\n",
    "def feature_extraction(training_data, method='sift', hog_vizualize=False ):\n",
    "    \n",
    "    if (method == 'sift'):\n",
    "        sift = cv2.SIFT_create()\n",
    "        keypoints, descriptors = [], []\n",
    "\n",
    "        for img in training_data:\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            kps, dss  = sift.detectAndCompute(gray, None)\n",
    "            keypoints.append(kps)\n",
    "            \n",
    "            # If dss not found fill np.array(1,128) with zeros \n",
    "            if dss is None:\n",
    "                dss = np.zeros((1,128), np.float32)\n",
    "            descriptors.append(dss)\n",
    "\n",
    "        return keypoints, descriptors\n",
    "    \n",
    "    if (method == 'hog'):\n",
    "        return None\n",
    "    \n",
    "def plot_sift_features(keypoints, training_data, data_labels):\n",
    "\n",
    "    # get indices p/ class\n",
    "    classes = np.unique(data_labels)\n",
    "    label_indices = [[] for _ in classes]\n",
    "    for i, label in enumerate(data_labels):\n",
    "        label_indices[label].append(i)\n",
    "    \n",
    "    row, col = 2, label_indices\n",
    "    fig, ax = plt.subplots(row, len(col), figsize=(12,5))\n",
    "    fig.suptitle(\"SIFT Keypoints on Training Data\", fontsize=14)\n",
    "    \n",
    "    \n",
    "    for r in range(row):\n",
    "        for c, index_set in enumerate(col):\n",
    "            \n",
    "            index = index_set[r]\n",
    "            curr_img = training_data[index]\n",
    "            img = cv2.drawKeypoints(curr_img, keypoints[index],None,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "            ax[r, c].imshow(img)\n",
    "            ax[r, c].set_title(dic_class_labels.get(c))\n",
    "            ax[r, c].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIFT Features extracted!\n"
     ]
    }
   ],
   "source": [
    "keypoints, train_descriptors = feature_extraction(training_data)\n",
    "print(\"SIFT Features extracted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sift_features(keypoints, training_data, training_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZ6OkCV9LIM9"
   },
   "source": [
    "## **2.2 Building Visual Vocabulary**\n",
    "\n",
    "Here, we will obtain visual words by clustering feature descriptors, so each cluster center is a visual word. Take a subset of all training images (this subset should contain images from ALL categories), extract SIFT descriptors from all of these images, and run k-means clustering (you can use your favourite k-means implementation) on these SIFT descriptors to build your visual vocabulary. Then, take the rest of the training images to calculate a visual dictionary.\n",
    "\n",
    "You can also use less images, say 100 from each class (exclusive from the previous subset) if your computational resources are limited (remember to analyze the effects). Pre-defined cluster numbers will be the size of your vocabulary. In this question, set its size to 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6a1z2vK99h7"
   },
   "source": [
    "####  **` Q2.2: Building Visual Vocabulary. (10-pts)`**\n",
    "Create a visual vocabulary by using K-means clustering. Remember to display the results when the vocabulary subset is 30\\%, 40\\% and 50\\% amount of the training images. The vocabulary size is fixed 1000 in this question.\n",
    "\n",
    "**Hint 1:** Remember first to debug all the code with a small amount of input images and only when you are sure that code functions correctly run it for training over the larger dataset.\n",
    "\n",
    "**Hint 2:** You can achieve K-means clustering using either *sklearn* package or *scipy* package.\n",
    "\n",
    "**Hint 3:** Results of K-means clustering could be shown using a scatter plot. In this case, the high-dimensional SIFT descriptors need to be brought down to a lower (2D) dimension  (*PCA* from *sklearn.decomposition* is one method to do this). For easy visualization, plotting up to 10 clusters would suffice. Check out an example implementation [here](https://www.askpython.com/python/examples/plot-k-means-clusters-python). Note that you are free to follow this approach, or you can show the K-means results in any other appropriate method that drives home your point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# YOUR CODE HERE\n",
    "######################\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "vocabulary_size=1000\n",
    "\n",
    "def mk_vocabulary(descriptors, data_labels, percentage_subset=0.3, method='sift', n_clusters=vocabulary_size):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # get indices p/ class\n",
    "    classes = np.unique(data_labels)\n",
    "    label_indices = [[] for _ in classes]\n",
    "    for i, label in enumerate(data_labels):\n",
    "        label_indices[label].append(i)\n",
    "\n",
    "    # reduce to desired percentage subset\n",
    "    subset_indices = []\n",
    "    for label_set in label_indices:\n",
    "        subset_indices += label_set[:int(1000 * percentage_subset)]\n",
    "        \n",
    "    # get subset descriptors \n",
    "    subset_dss = []\n",
    "    for i in subset_indices:\n",
    "        subset_dss += [descriptors[i]]\n",
    "\n",
    "    if (method == 'sift'):\n",
    "        subset_dss = np.vstack(subset_dss)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init='auto').fit(subset_dss)\n",
    "\n",
    "    return kmeans, subset_dss, subset_indices\n",
    "\n",
    "def plot_clusters(kmeans, subset_dss, percentage_subset):\n",
    "\n",
    "    # project to new lower dimension\n",
    "    pca = PCA(2)\n",
    "    transformed_data = pca.fit_transform(subset_dss)\n",
    "\n",
    "    # compute the means per cluster\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    means = pca.transform(centroids)\n",
    "\n",
    "    # get labels for first 10 clusters\n",
    "    n_clusters = 10\n",
    "    cluster_sets = kmeans.predict(subset_dss)\n",
    "    labels = np.unique(cluster_sets)[:n_clusters]\n",
    "    \n",
    "    # plot means per cluster\n",
    "    plt.scatter(means[:n_clusters, 0], means[:n_clusters, 1], color='k', s=75, marker= 'x', linewidths=2)\n",
    "\n",
    "    # plot transformed data\n",
    "    for i in range(n_clusters):\n",
    "        plt.scatter(transformed_data[cluster_sets == i , 0] , transformed_data[cluster_sets == i , 1], label=labels[i],  alpha=0.25)\n",
    "\n",
    "    plt.title(f\"10 Cluster Centers using {int(percentage_subset*100)}% of Descriptors\", weight='bold')\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.grid(alpha=0.2)\n",
    "    plt.show()\n",
    "\n",
    "# kmeans_30, subset_dss_30, subset_indices_30 = mk_vocabulary(train_descriptors, training_label, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_30, subset_dss_30, subset_indices_30 = mk_vocabulary(train_descriptors, training_label, 0.3)\n",
    "kmeans_40, subset_dss_40, subset_indices_40 = mk_vocabulary(train_descriptors, training_label, 0.4)\n",
    "kmeans_50, subset_dss_50, subset_indices_50 = mk_vocabulary(train_descriptors, training_label, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(kmeans_30, subset_dss_30, 0.3)\n",
    "plot_clusters(kmeans_40, subset_dss_40, 0.4)\n",
    "plot_clusters(kmeans_50, subset_dss_50, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gihEWG9MC8b8"
   },
   "source": [
    "## **2.3 Encoding Features Using Visual Vocabulary**\n",
    "\n",
    "Once we have a visual vocabulary, we can represent each image as a collection of visual words. For this purpose, we need to extract feature descriptors (with SIFT) and then assign each descriptor to the closest visual word from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Vu6NaYtRzbH2"
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# YOUR CODE HERE\n",
    "######################\n",
    "def encode_features(descriptors, data_labels, kmeans, method='sift'):\n",
    "    \n",
    "    visual_words, labels = [], []\n",
    "    # iterate over dss and labels in parallel\n",
    "    for img_dss, label in zip(descriptors, data_labels):\n",
    "\n",
    "        if (method=='sift'):\n",
    "            closest_words = kmeans.predict(img_dss)\n",
    "        \n",
    "        elif (method=='hog'):\n",
    "            temp = 0\n",
    "        \n",
    "        visual_words.append(closest_words)\n",
    "        labels.append(label)\n",
    "\n",
    "    return visual_words, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_words, vw_labels = encode_features(train_descriptors, training_label, kmeans_30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9GRPT4kDGRt"
   },
   "source": [
    "## **2.4 Representing images by frequencies of visual words**\n",
    "\n",
    "The next step is the quantization. The idea is to represent each image by a histogram of its visual words. Check out matplotlib's [hist()](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html) function. Since different images can have different numbers of features, histograms should be normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brdvDtpTDeoA"
   },
   "source": [
    "####  **` Q2.4: Representing images by frequencies of visual words. (5-pts)`**\n",
    "\n",
    "Pick a subset ratio from the above settings (30%, 40% and 50%). For each class, calculate the frequency of visual words. Visualize the visual words' frequency by histogram. The X-axis is supposed to be the visual word number (e.g.,from 0 to 1000), and the Y-axis is supposed to be the frequency. Describe the similarities and differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ER-moXF99h9"
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# YOUR CODE HERE\n",
    "################################\n",
    "def plot_histogram(visual_words, labels, percentage_subset):\n",
    "    '''dic_class_labels = {0: 'airplanes', \n",
    "                            1: 'birds', \n",
    "                            2: 'ships', \n",
    "                            3: 'horses', \n",
    "                            4: 'automobiles'}\n",
    "    '''\n",
    "    np.random.seed(42)\n",
    "    bins = range(vocabulary_size)\n",
    "\n",
    "    for i_class, i_label in dic_class_labels.items():\n",
    "\n",
    "        # calculate new indices per each class\n",
    "        indices = []\n",
    "        for index, label in enumerate(labels):\n",
    "            if i_class == label:\n",
    "                indices.append(index)\n",
    "\n",
    "        counts = np.zeros(vocabulary_size)\n",
    "        # Go over 1000-kth data \n",
    "        for i in indices:\n",
    "\n",
    "            # Go over 10 clusters\n",
    "            for word in visual_words[i]:\n",
    "                counts[word] += 1\n",
    "\n",
    "        norm_counts =  counts / np.max(counts)\n",
    "        \n",
    "        # plotting\n",
    "        plt.figure(figsize=(5, 3))\n",
    "        plt.bar(bins, norm_counts)\n",
    "        plt.title(f\"Histogram for class: {i_label.capitalize()} w/ {int(percentage_subset*100)}% Descriptors\")\n",
    "        plt.xlabel(\"Visual Word Number\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.grid(alpha=0.1)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(visual_words, vw_labels, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzvBkirXDubr"
   },
   "source": [
    "## **2.5 Classification**\n",
    "\n",
    "We will train a classifier per each object class. Now, we take the Support Vector Machine (SVM) as an example. As a result, we will have 5 *binary* classifiers.\n",
    "\n",
    "Take images from the training set of the related class (should be the ones which you did not use for dictionary calculation). Represent them with histograms of visual words as discussed in the previous section. Use at least 50 training images per class or more, but remember to debug your code first! If you use the default setting, you should have 50 histograms of size 1000. These will be your positive examples.\n",
    "\n",
    "Then, you will obtain histograms of visual words for images from other classes, again about 50 images per class, as negative examples. Therefore, you will have 200 negative examples.\n",
    "\n",
    "Now, you are ready to train a classifier. You should repeat it for each class. To classify a new image, you should calculate its visual words histogram as described in Section 2.4 and use the trained SVM classifier to assign it to the most probable object class. (Note that for proper SVM scores you need to use cross-validation to get a proper estimate of the SVM parameters. In this assignment, you do not have to experiment with this cross-validation step)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaZtBMX9D7UR"
   },
   "source": [
    "####  **` Q2.5: Classification (5-pts)`**\n",
    "\n",
    "Utilize SVM and finish classification training.\n",
    "\n",
    "**Hint:**\n",
    "You can use *scikit-learn* software to conduct SVM classification. The relevant documents can be found at this [link](https://scikit-learn.org/stable/modules/svm.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BvuuuJBt99h-"
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# YOUR CODE HERE\n",
    "################################\n",
    "from sklearn.svm import SVC\n",
    "import random\n",
    "\n",
    "def get_histograms(visual_words, vocabulary_size=1000):\n",
    "    histograms = []\n",
    "    for words in visual_words:\n",
    "        if words is None:\n",
    "            return print(\"Debug at: get_histograms()!\")\n",
    "        hist, _ = np.histogram(words, bins=vocabulary_size, density=True)\n",
    "        histograms.append(hist)\n",
    "    return histograms\n",
    "\n",
    "def train_SVM(training_data, training_labels, kmeans, percentage_subset=0.3, n_samples=50, vocabulary_size=1000):\n",
    "\n",
    "    classes = np.unique(training_labels)\n",
    "    svm_models = {}\n",
    "\n",
    "    for class_number in classes:\n",
    "\n",
    "        p_data = [img for img, label in zip(training_data, training_labels) if label == class_number]\n",
    "        n_data = [img for img, label in zip(training_data, training_labels) if label != class_number]\n",
    "        \n",
    "        p_examples = random.sample(p_data, 50)\n",
    "        n_examples = random.sample(n_data, 200)\n",
    "\n",
    "        _, p_descriptors = feature_extraction(p_data)\n",
    "        _, n_descriptors = feature_extraction(n_data)\n",
    "\n",
    "        p_visual_words, _ = encode_features(p_descriptors, training_labels[:50], kmeans)\n",
    "        n_visual_words, _ = encode_features(n_descriptors, training_labels[:200], kmeans)\n",
    "\n",
    "        p_histograms = get_histograms(p_visual_words, vocabulary_size)\n",
    "        n_histograms = get_histograms(n_visual_words, vocabulary_size)\n",
    "\n",
    "        X_train = p_histograms + n_histograms\n",
    "        y_train = [1] * len(p_examples) + [0] * len(n_examples)\n",
    "\n",
    "        m = SVC(probability=True, class_weight=\"balanced\").fit(X_train, y_train)\n",
    "        svm_models[class_number] = m\n",
    "\n",
    "    return svm_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VT2FNr3rET3l"
   },
   "source": [
    "## **2.6 Evaluation**\n",
    "\n",
    "To evaluate your system, you should take all the test images from all classes and rank them based on each binary classifier. In other words, you should classify each test image with each classifier and then sort them based on the classification score. As a result, you will have five lists of test images. Ideally, you would have images with airplanes on the top of your list which is created based on your airplane classifier, and images with cars on the top of your list which is created based on your car classifier, and so on.\n",
    "\n",
    "In addition to the qualitative analysis, you should measure the performance of the system quantitatively with the Mean Average Precision (mAP) over all classes. The mAP for a single class c is defined as\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{1}{m_c} \\sum_{i=1}^{n} \\frac{f_c(x_i)}{i}\n",
    "\\end{equation}\n",
    "$$\n",
    "where $n$ is the number of images ($n=50\\times 5=250$), $m$ is the number of images of class $c$  ($m_c=50$), $x_i$ is the $i^{th}$ image in the ranked list $X = \\left \\{ x_1, x_2, \\dots, x_n  \\right \\}$, and finally, $f_c$ is a function which returns the number of images of class $c$ in the first $i$ images if $x_i$ is of class $c$, and 0 otherwise. To illustrate, if we want to retrieve $R$ and we get the following sequence: $[R, R, T, R, T, T, R, T]$, then $n = 8$, $m = 4$, and $AP(R, R, T, R, T, T, R) = \\frac{1}{4} \\left (  \\frac{1}{1} + \\frac{2}{2} + \\frac{0}{3} + \\frac{3}{4} + \\frac{0}{5} + \\frac{0}{6} + \\frac{4}{7} + \\frac{0}{8} \\right )$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cjfUXneEubT"
   },
   "source": [
    "####  **` Q2.6: Evaluation and Discussion (30-pts)`**\n",
    "\n",
    "Show the evaluation results and describe. The report should include the analysis of the results for different settings such as:\n",
    "- mAP based on different subset ratios to create the vocabulary list (30%, 40% and 50%) under the fixed vocabulary size 1000.\n",
    "- Based on the ratio among the above four settings that lead to the best performance, change the vocabulary sizes to different sizes (500, 1000 and 1500). Report and discuss the mAP.\n",
    "- Based on the above experiments, find the best setting. Report the mAP based on SIFT descriptor and HoG descriptor.\n",
    "- Visualize the top-5 and the bottom-5 ranked test images (based on the classifier confidence for the target class) under the best setting.\n",
    "- The impact of the hyper-parameters of SVM.  \n",
    "\n",
    "**Hint 1:**\n",
    "To alleviate the working load, the discussion on the impact of SVM’s hyper-parameter settings only need to based on the optimal settings from the first three questions.\n",
    "\n",
    "**Hint 2:**\n",
    "Be sure to discuss the differences between different settings such as vocabulary sizes in your report.\n",
    "\n",
    "**Hint 3:**\n",
    "You can use *skimage.feature.hog* to extract HoG descriptor. The relevant documents can be found at [link](https://scikit-image.org/docs/dev/api/skimage.feature.html?highlight=hog#skimage.feature.hog)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "qeCMyWD3FSIB"
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# YOUR CODE HERE\n",
    "################################\n",
    "\n",
    "def compute_MAP_scores(y_pred, y_true):\n",
    "    AP_classes = []\n",
    "    for class_predictions, class_true in zip(y_pred, y_true):\n",
    "        summation = 0\n",
    "        for i in range(len(class_predictions)):\n",
    "            if class_true[i] == 1:\n",
    "                summation += len([pred for pred in class_predictions[:i+1] if pred == 1]) / (i+1)\n",
    "        AP_score = (1/len([label for label in class_true if label == 1])) * summation\n",
    "        AP_classes.append(AP_score)\n",
    "    \n",
    "    map_score = np.sum(AP_classes) / len(AP_classes)\n",
    "\n",
    "    return map_score, AP_classes\n",
    "\n",
    "def calculate_map(test_label, sort_indices, cur_class):\n",
    "    sorted_labels = test_label[sort_indices]\n",
    "    class_labels = np.where(sorted_labels==cur_class, 1, 0).astype('float32')\n",
    "    m = np.sum(class_labels)\n",
    "    num_correct = 0.0\n",
    "    map_sum = 0.0\n",
    "    for idx in range(len(class_labels)):\n",
    "        if(class_labels[idx]):\n",
    "            num_correct = num_correct + 1.0\n",
    "            map_sum = map_sum + (num_correct/(idx+1.0))\n",
    "    return map_sum/m\n",
    "\n",
    "\n",
    "def test_SVM(svm_models, test_data, test_label, kmeans, vocabulary_size=1000):\n",
    "\n",
    "    _, test_descriptors = feature_extraction(test_data)\n",
    "    test_visual_words, _ = encode_features(test_descriptors, test_label, kmeans)\n",
    "    test_histograms = get_histograms(test_visual_words, vocabulary_size)\n",
    "\n",
    "    y_pred, y_true = [], []\n",
    "    classes = dic_class_labels.keys()\n",
    "\n",
    "    class_maps = {class_number: [] for class_number in classes}\n",
    "\n",
    "    for class_number in classes:\n",
    "\n",
    "        clf = svm_models[class_number]\n",
    "        predictions = clf.predict_proba(test_histograms)\n",
    "        sort_indices = np.argsort(predictions[:,0])\n",
    "\n",
    "        class_maps[class_number] = calculate_map(test_label, sort_indices, class_number)\n",
    "\n",
    "        # predictons = svm_models[class_number].predict(test_histograms)\n",
    "        # y_pred.append(predictons) \n",
    "        # y_true.append(test_label)\n",
    "\n",
    "        # sorted_indices = np.flip(np.argsort(predictons))\n",
    "\n",
    "        # print(calculate_map(predictons, sorted_indices,class_number))\n",
    "\n",
    "        # y_pred.append(predictons[sorted_indices])\n",
    "        # y_true.append(np.array(test_label)[sorted_indices])\n",
    "\n",
    "    print(class_maps)\n",
    "\n",
    "    # return y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.3018911014922775, 1: 0.16990689077035398, 2: 0.2818520667668375, 3: 0.13429514960307315, 4: 0.14348330741478219}\n",
      "{0: 0.2242853097830342, 1: 0.17721871116297036, 2: 0.1458417710825519, 3: 0.1348418872634549, 4: 0.14334549234477728}\n",
      "{0: 0.15191366207653714, 1: 0.17426101501268565, 2: 0.2813237774561906, 3: 0.13494572334932742, 4: 0.14267861753834626}\n"
     ]
    }
   ],
   "source": [
    "#########################################################\n",
    "\n",
    "trained_kmeans = [kmeans_30, kmeans_40, kmeans_50]\n",
    "subset_indices = [subset_indices_30, subset_indices_40, subset_indices_50]\n",
    "n_test_classes = np.unique(test_label).size\n",
    "\n",
    "for i, kmean in enumerate(trained_kmeans):\n",
    "    \n",
    "    # get non-used imgs \n",
    "    non_used_imgs = np.delete(np.arange(training_data.shape[0]), subset_indices[i])\n",
    "    non_used_training_data, non_used_training_labels = training_data[non_used_imgs], training_label[non_used_imgs]\n",
    "    \n",
    "    svms = train_SVM(non_used_training_data, non_used_training_labels, kmean)\n",
    "    test_SVM(svms, test_data, test_label, kmean)\n",
    "\n",
    "    # MAP_score, MAP_per_class = compute_MAP_scores(y_pred, y_true)\n",
    "    # print(MAP_score, MAP_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2488974162524898\n",
      "0.16980716095509524\n",
      "0.24229361345163092\n",
      "0.17105532093063255\n",
      "0.19210359000073038\n",
      "{0: 0.15191366207653714, 1: 0.17426101501268565, 2: 0.2813237774561906, 3: 0.13494572334932742, 4: 0.14267861753834626}\n"
     ]
    }
   ],
   "source": [
    "test_SVM(svms, test_data, test_label, kmean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grbC046Fgcs0"
   },
   "source": [
    " # Individual Contribution Report *(Mandatory)*\n",
    "\n",
    "Because we want each student to contribute fairly to the submitted work, we ask you to fill out the textcells below. Write down your contribution to each of the assignment components in percentages. Naturally, percentages for one particular component should add up to 100% (e.g. 30% - 30% - 40%). No further explanation has to be given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_I5rcHbegcs0"
   },
   "source": [
    "Name:\n",
    "\n",
    "Contribution on research: \\\n",
    "Contribution on programming: \\\n",
    "Contribution on writing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KnwDXcbgcs0"
   },
   "source": [
    "Name:\n",
    "\n",
    "Contribution on research: \\\n",
    "Contribution on programming: \\\n",
    "Contribution on writing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKSXrsjygcs0"
   },
   "source": [
    "Name:\n",
    "\n",
    "Contribution on research: \\\n",
    "Contribution on programming: \\\n",
    "Contribution on writing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NX-LCuYUgcs0"
   },
   "source": [
    "Name:\n",
    "\n",
    "Contribution on research: \\\n",
    "Contribution on programming: \\\n",
    "Contribution on writing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RctQ8Z1Cgcs0"
   },
   "source": [
    " # -End of Notebook-"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
